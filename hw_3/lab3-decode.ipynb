{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MT Lab 3: Decoder for edit phrase table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TranslationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a `phrase` type for the possibly translated phrase with phrase itself and the probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = namedtuple(\"phrase\", \"text, logprob\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your translation model loading function. We recommend you make use of the `phrase` type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tm(filename, topK = 20):\n",
    "    # write your code here...\n",
    "    tm = {}\n",
    "    for line in open(filename):\n",
    "        (f, e, logprob) = line.strip().split(\" ||| \")\n",
    "        tm.setdefault(tuple(f.split()), []).append(phrase(e, float(logprob)))\n",
    "    for f in tm:  # prune all but top k translations\n",
    "        tm[f].sort(key=lambda x: -x.logprob)\n",
    "        del tm[f][topK:]\n",
    "    return tm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load translation model (edit phrase table in lab2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm = load_tm('phrase_table.dev.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test if your tm is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[phrase(text='discuss about', logprob=-0.4721),\n",
       " phrase(text='discuss', logprob=-1.3376)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm['discuss', 'about']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use KenLM as our language model for decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kenlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm = kenlm.Model('bnc.prune.bin')\n",
    "lm = kenlm.Model('/home/nlplab/jjc/1b.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-12.276196479797363"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.score(\"We can discuss about it .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9.702674865722656"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.score(\"We can discuss it .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-13.259819030761719"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.score(\"discuss about\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7.445827007293701"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.score(\"discuss about\", bos=False, eos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'kenlm.Model' object has no attribute 'end'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-eeb8efd4cd29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'kenlm.Model' object has no attribute 'end'"
     ]
    }
   ],
   "source": [
    "lm.end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_sents = [tuple(line.strip().split()) for line in open('src.txt')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can initialize unknown words as-is with a default log probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init unknown words probabilty for tm here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or do it later in the decoding process. (use `dict.get`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code implements a monotone decoding\n",
    "# algorithm (one that doesn't permute the target phrases).\n",
    "# Hence all hypotheses in stacks[i] represent translations of \n",
    "# the first i words of the input sentence. You should generalize\n",
    "# this so that they can represent translations of *any* i words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a `hypothesis` type for the possibly translated phrase with phrase itself and the probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = namedtuple(\"hypothesis\", \"logprob, tmprob, state, predecessor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "devdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_sent = 'We can discuss about it .'\n",
    "src_sent = tuple(dev_sent.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('We',)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_sent[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your decode algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRUN = 10\n",
    "# for line in src_sent:\n",
    "initial_hypothesis = hypothesis(0.0, 0.0, '', None)\n",
    "stacks = [{} for _ in range(len(src_sent)+1)]\n",
    "stacks[0][''] = initial_hypothesis\n",
    "\n",
    "for i, stack in enumerate(stacks[:-1]):\n",
    "    for h in sorted(stack.values(), key=lambda h: -h.logprob)[:PRUN]:\n",
    "        for j in range(i+1, len(src_sent)+1):\n",
    "            if src_sent[i:j] in tm:\n",
    "                for phrase in tm[f[i:j]]:\n",
    "                    logprob = h.logprob + phrase.logprob\n",
    "                    lm_state = h.lm_state\n",
    "                    for word in phrase.text.split():\n",
    "                        (lm_state, word_logprob) = lm.score(lm_state, word)\n",
    "                        logprob += word_logprob\n",
    "                    new_hypothesis = hypothesis(logprob, lm_state, h, phrase)\n",
    "                    if lm_state not in stacks[j] or stacks[j][lm_state].logprob < logprob:  \n",
    "                        # second case is recombination\n",
    "                        # if new logprob is much better\n",
    "                        stacks[j][lm_state] = new_hypothesis\n",
    "    \n",
    "    \n",
    "    \n",
    "    # write your code here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "select the one with highest score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "winner = max(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pirnt decode result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We can discuss about it .\n",
      "We can discuss it .\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(src_sent))\n",
    "print(winner.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LM = -9.702674865722656, TM = -5.3376, Total = -15.040274865722656\n"
     ]
    }
   ],
   "source": [
    "print(\"LM = {}, TM = {}, Total = {}\".format(lm.score(winner.state), winner.tmprob, winner.logprob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
